{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a Modified Copy of the Basic Example with Most Cells Merged\n",
    "\n",
    "This is a modified copy of the basic example of a Single and Multilayer Perceptron for Classification and Regression.\n",
    "\n",
    "It will allow to test different scenarios by changing easily the global variables.\n",
    "\n",
    "**TO SIMPLIFY, ONLY CLASSIFICATION PROBLEMS ARE TESTED**\n",
    "\n",
    "**TO SIMPLIFY, MOST CELLS ARE MERGED**\n",
    "\n",
    "**This script has been tested with the following package versions:**\n",
    "- pandas 1.3.3\n",
    "- sklearn 0.24.0\n",
    "- keras 2.2.4 + tensorflow 1.14.0 / keras 2.6.0 + tensorflow 2.6.0\n",
    "\n",
    "**Maybe you can activate a conda environment already created:**\n",
    "- conda create --name masternn python=3.9\n",
    "- conda activate masternn\n",
    "- conda install jupyter matplotlib pandas\n",
    "- pip install sklearn keras==2.6.0 tensorflow==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataFunctions                ### Functions for data management\n",
    "import numpy                        ### Library for numerical computations\n",
    "import keras, tensorflow, sklearn   ### Libraries for constructing and training the models\n",
    "import matplotlib.pyplot as plt     ### Library for plotting\n",
    "import copy                         ### Allows copy and deepcopy\n",
    "print(keras.__version__)\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Main function\n",
    "\n",
    "def mainFunction(params):\n",
    "\n",
    "    ### First we read inputs and labels\n",
    "    x, y = DataFunctions.loadDatasetsFromFiles (params.INPUTSFILENAME, params.LABELSFILENAME)\n",
    "    y    = y.ravel()   ### sklearn prefers shapes (N,) than (N,1)\n",
    "\n",
    "    ### Maybe we want to add random features\n",
    "    if params.NRANDOM_FEATURES != 0:\n",
    "        xx = numpy.random.rand(x.shape[0],params.NRANDOM_FEATURES)\n",
    "        x  = numpy.hstack([x, xx])\n",
    "\n",
    "    ### Maybe we want to add random examples\n",
    "    if params.NRANDOM_EXAMPLES != 0:\n",
    "        xx = numpy.random.rand(params.NRANDOM_EXAMPLES,x.shape[1])\n",
    "        x  = numpy.vstack([x, xx])\n",
    "        yy = numpy.random.randint(numpy.min(y),numpy.max(y)+1,params.NRANDOM_EXAMPLES)\n",
    "        y  = numpy.hstack([y, yy])\n",
    "\n",
    "    ### Maybe we want to add noise to the data\n",
    "    if params.ADD_NOISE_INPUTS:\n",
    "        x += numpy.random.randn(x.shape[0],x.shape[1])\n",
    "\n",
    "    ### Maybe we want to shuffle the labels\n",
    "    if params.SHUFFLE_LABELS:\n",
    "        random.shuffle(y)\n",
    "\n",
    "    nExamples = x.shape[0]\n",
    "    nFeatures = x.shape[1]\n",
    "    nClasses  = len(numpy.unique(y))                   ### only for CLASSIFICATION\n",
    "\n",
    "    ### Convert labels to a 1-of-C (one-hot) scheme\n",
    "    ## For neural networks, it is easier to output yes/no than (for example) an integer with the predicted class\n",
    "    y1C = DataFunctions.convertLabels_1ofC_Scheme (y)  ### only for CLASSIFICATION\n",
    "    \n",
    "    ### Scale inputs\n",
    "    if   params.SCALE_INPUTS_FUNCTION == \"M0SD1\":\n",
    "        x, Scaler = DataFunctions.scaleDataMean0Dev1Scaler (x)\n",
    "    elif params.SCALE_INPUTS_FUNCTION == \"MinMax\":\n",
    "        x, Scaler = DataFunctions.scaleDataMinMaxScaler (x, FeatureRange=(-1,+1))\n",
    "    #print(\"First 3 rows of x:\"); print(x[0:3,])\n",
    "\n",
    "    ### Split data into training (to construct the model) and test (to estimate the generalization)\n",
    "    from sklearn import model_selection\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "      model_selection.train_test_split (x, y, train_size=params.TRAIN_SIZE_PCT_SPLIT, shuffle=True, stratify=y)\n",
    "    y1C_train = DataFunctions.convertLabels_1ofC_Scheme (y_train)  ### only for CLASSIFICATION\n",
    "    y1C_test  = DataFunctions.convertLabels_1ofC_Scheme (y_test)   ### only for CLASSIFICATION\n",
    "\n",
    "    ###\n",
    "    ### https://keras.io/api/models/sequential/#sequential-class\n",
    "    ###\n",
    "\n",
    "    ### First we indicate that it is a sequential model\n",
    "    myNetwork = keras.Sequential()\n",
    "\n",
    "    ### We need to indicate the input dimension in the first layer\n",
    "    inputDimension  = nFeatures\n",
    "    outputDimension = nClasses    ### only for CLASSIFICATION\n",
    "\n",
    "    if params.MULTILAYER_PERCEPTRON:\n",
    "\n",
    "        ### Now we add the hidden layers\n",
    "        myNetwork.add ( keras.layers.Dense (params.NHIDDEN1, activation=params.FACTIVATION_HIDDEN1, input_dim=inputDimension) )\n",
    "        #myNetwork.add ( keras.layers.Dense (params.NHIDDEN1, activation=params.FACTIVATION_HIDDEN1, kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01), input_dim=nFeatures) )\n",
    "        if params.NHIDDEN2 != 0:\n",
    "            myNetwork.add ( keras.layers.Dense (params.NHIDDEN2, activation=params.FACTIVATION_HIDDEN2) )\n",
    "            #myNetwork.add ( keras.layers.Dense (params.NHIDDEN2, activation=params.FACTIVATION_HIDDEN2 kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01)) )\n",
    "        if params.NHIDDEN3 != 0:\n",
    "            myNetwork.add ( keras.layers.Dense (params.NHIDDEN3, activation=params.FACTIVATION_HIDDEN3) )\n",
    "            #myNetwork.add ( keras.layers.Dense (params.NHIDDEN3, activation=params.FACTIVATION_HIDDEN3 kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01)) )\n",
    "        if params.NHIDDEN4 != 0:\n",
    "            myNetwork.add ( keras.layers.Dense (params.NHIDDEN4, activation=params.FACTIVATION_HIDDEN4) )\n",
    "            #myNetwork.add ( keras.layers.Dense (params.NHIDDEN4, activation=params.FACTIVATION_HIDDEN4 kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01)) )\n",
    "        if params.NHIDDEN5 != 0:\n",
    "            myNetwork.add ( keras.layers.Dense (params.NHIDDEN5, activation=params.FACTIVATION_HIDDEN5) )\n",
    "            #myNetwork.add ( keras.layers.Dense (params.NHIDDEN5, activation=params.FACTIVATION_HIDDEN5 kernel_regularizer=keras.regularizers.l2(0.01), bias_regularizer=keras.regularizers.l2(0.01)) )\n",
    "\n",
    "        ### And finally we add the output layer\n",
    "        myNetwork.add ( keras.layers.Dense (outputDimension, activation=params.FACTIVATION_OUTPUT) )\n",
    "\n",
    "    else:\n",
    "\n",
    "        ### We only have an output layer\n",
    "        myNetwork.add ( keras.layers.Dense (outputDimension, activation=params.FACTIVATION_OUTPUT, input_dim=inputDimension) )\n",
    "\n",
    "    ### Print statistics\n",
    "    print(myNetwork.summary())\n",
    "\n",
    "    ### Now we create a keras model\n",
    "    myInput = keras.layers.Input (shape=(nFeatures,))\n",
    "    myModel = keras.models.Model (inputs=myInput, outputs=myNetwork(myInput))\n",
    "\n",
    "    ### Loss function\n",
    "    ## Usual loss functions: 'categorical_crossentropy' 'binary_crossentropy' 'mean_squared_error', etc\n",
    "    lossFunction = ['categorical_crossentropy']   ### only for CLASSIFICATION (for one-hot labels, use categorical_crossentropy)\n",
    "\n",
    "    #print(keras.__version__)\n",
    "    if keras.__version__ < \"2.3.0\":\n",
    "        optimizers = keras.optimizers\n",
    "    else:\n",
    "          optimizers = tensorflow.keras.optimizers\n",
    "\n",
    "    ### Training algorithm\n",
    "    ## Every training algorithm will have its own parameters\n",
    "    if   params.TRAINING_ALGORITHM == \"SGD\":\n",
    "        trainAlgorithm = optimizers.SGD (lr=params.LEARNING_RATE, momentum=params.MOMENTUM_RATE)  # There are more parameters\n",
    "    elif params.TRAINING_ALGORITHM == \"RMSprop\":\n",
    "        trainAlgorithm = optimizers.RMSprop (lr=params.LEARNING_RATE)                             # There are more parameters\n",
    "    elif params.TRAINING_ALGORITHM == \"Adam\":\n",
    "        trainAlgorithm = optimizers.Adam (lr=params.LEARNING_RATE)                                # There are more parameters\n",
    "\n",
    "    ### Metrics to monitorize\n",
    "    ## Keras allows to monitorize several metrics along training\n",
    "    showMetrics = ['categorical_accuracy', 'categorical_crossentropy', 'mean_squared_error']\n",
    "\n",
    "    ### Compile the model with all the elements (this is the standard way to work in keras)\n",
    "    myModel.compile (loss=lossFunction, optimizer=trainAlgorithm, metrics=showMetrics)\n",
    "\n",
    "    ###\n",
    "    ### This method has many parameters:\n",
    "    ###   https://keras.io/api/models/model_training_apis/#fit-method\n",
    "    ###\n",
    "\n",
    "    validationData = (x_test,y1C_test)  ### We could also use the validation_split parameter\n",
    "    fitData = myModel.fit \\\n",
    "      (x_train, y1C_train, validation_data=validationData, batch_size=params.BATCHSIZE, epochs=params.NEPOCHS)\n",
    "\n",
    "    return myModel, fitData, x_train, y1C_train, x_test, y1C_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### This class is only used to set default parameters\n",
    "\n",
    "class defaultParameters(object):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.MULTILAYER_PERCEPTRON = True      ### If False, it is a Single-layer Perceptron\n",
    "\n",
    "        self.NRANDOM_EXAMPLES      = 0         ### Number of random examples to add\n",
    "        self.NRANDOM_FEATURES      = 0         ### Number of random features to add\n",
    "        self.ADD_NOISE_INPUTS      = False     ### If True, add standard Gaussian noise to the inputs\n",
    "        self.SHUFFLE_LABELS        = False     ### If True, shuffle the labels (without shuffling the inputs)\n",
    "        self.SCALE_INPUTS_FUNCTION = \"M0SD1\"   ### \"M0SD1\": Mean 0 StdDev 1 / \"MinMax\": Values in an interval / None\n",
    "\n",
    "        self.TRAIN_SIZE_PCT_SPLIT  = 0.70      ### Percentage of data used for training (it must be in (0,1])\n",
    "\n",
    "        self.NHIDDEN1 = 100;    self.FACTIVATION_HIDDEN1 = 'tanh'\n",
    "        self.NHIDDEN2 = 50;     self.FACTIVATION_HIDDEN2 = 'tanh'\n",
    "        self.NHIDDEN3 = 0;      self.FACTIVATION_HIDDEN3 = 'tanh'\n",
    "        self.NHIDDEN4 = 0;      self.FACTIVATION_HIDDEN4 = 'tanh'\n",
    "        self.NHIDDEN5 = 0;      self.FACTIVATION_HIDDEN5 = 'tanh'\n",
    "        self.FACTIVATION_OUTPUT = 'softmax'    ### only for CLASSIFICATION\n",
    "\n",
    "        self.TRAINING_ALGORITHM = \"SGD\"        ### \"SGD\", \"RMSprop\", \"Adam\"\n",
    "\n",
    "        self.LEARNING_RATE      = 0.001        ### (almost) ALL training algorithms have a learning rate\n",
    "        self.MOMENTUM_RATE      = 0.80         ### Maybe not needed in some training algorithms\n",
    "\n",
    "        self.BATCHSIZE          = 20           ### Mini-batch size\n",
    "        self.NEPOCHS            = 200          ### Number of training iterations\n",
    "\n",
    "        self.INPUTSFILENAME = 'Data/ionosphere.inputs'\n",
    "        self.LABELSFILENAME = 'Data/ionosphere.labels'\n",
    "        #self.INPUTSFILENAME = 'Data/hepatitis.inputs'\n",
    "        #self.LABELSFILENAME = 'Data/hepatitis.labels'\n",
    "        #self.INPUTSFILENAME = 'Data/sonar.inputs'\n",
    "        #self.LABELSFILENAME = 'Data/sonar.labels'\n",
    "        #self.INPUTSFILENAME = 'Data/xor.inputs'\n",
    "        #self.LABELSFILENAME = 'Data/xor.labels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults1(myModel, fitData, x_train, y1C_train, x_test, y1C_test):\n",
    "    \n",
    "    ### Training history\n",
    "    lossTrain = fitData.history[\"loss\"]\n",
    "    lossValid = fitData.history[\"val_loss\"]\n",
    "\n",
    "    ### Plot\n",
    "    plt.figure(figsize=(7,5))\n",
    "    epochsPlot = range(1,len(lossTrain)+1)\n",
    "    plt.plot(epochsPlot,lossTrain,label='Training Loss')\n",
    "    plt.plot(epochsPlot,lossValid,label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ### Loss function and accuracy\n",
    "    scoresTrain = myModel.evaluate (x_train,y1C_train)\n",
    "    scoresTest  = myModel.evaluate (x_test,y1C_test)\n",
    "    print(\"Loss function value and Accuracy training set: %.5f  %7.3f%%\" % (scoresTrain[0], 100*scoresTrain[1])) \n",
    "    print(\"Loss function value and Accuracy test set:     %.5f  %7.3f%%\" % (scoresTest[0],  100*scoresTest[1]))\n",
    "\n",
    "def showResults2(myModel1, fitData1, x_train1, y1C_train1, x_test1, y1C_test1,\n",
    "                 myModel2, fitData2, x_train2, y1C_train2, x_test2, y1C_test2):\n",
    "\n",
    "    ### Loss function and accuracy 1\n",
    "    scoresTrain1 = myModel1.evaluate (x_train1,y1C_train1)\n",
    "    scoresTest1  = myModel1.evaluate (x_test1,y1C_test1)\n",
    "\n",
    "    ### Training history 1\n",
    "    lossTrain1 = fitData1.history[\"loss\"]\n",
    "    lossValid1 = fitData1.history[\"val_loss\"]\n",
    "\n",
    "    ### Loss function and accuracy 2\n",
    "    scoresTrain2 = myModel2.evaluate (x_train2,y1C_train2)\n",
    "    scoresTest2  = myModel2.evaluate (x_test2,y1C_test2)\n",
    "    \n",
    "    ### Training history 2\n",
    "    lossTrain2 = fitData2.history[\"loss\"]\n",
    "    lossValid2 = fitData2.history[\"val_loss\"]\n",
    "\n",
    "    ### Plot\n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    ### Plot 1\n",
    "    plt.subplot(1,2,1)\n",
    "    epochsPlot1 = range(1,len(lossTrain1)+1)\n",
    "    plt.plot(epochsPlot1,lossTrain1,label='Training Loss 1')\n",
    "    plt.plot(epochsPlot1,lossValid1,label='Validation Loss 1')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    ### Plot 2\n",
    "    plt.subplot(1,2,2)\n",
    "    epochsPlot2 = range(1,len(lossTrain2)+1)\n",
    "    plt.plot(epochsPlot2,lossTrain2,label='Training Loss 2')\n",
    "    plt.plot(epochsPlot2,lossValid2,label='Validation Loss 2')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    ### Print accuracies\n",
    "    print(\"   Loss function value and Accuracy\")\n",
    "    print()\n",
    "    print(\"        Training set 1: %.5f  %7.3f%%                      Training set 2: %.8f  %7.3f%%\" %\n",
    "          (scoresTrain1[0], 100*scoresTrain1[1], scoresTrain2[0], 100*scoresTrain2[1])) \n",
    "    print(\"        Test set 1:     %.5f  %7.3f%%                      Test set 2:     %.8f  %7.3f%%\" % \n",
    "          (scoresTest1[0],  100*scoresTest1[1], scoresTest2[0],  100*scoresTest2[1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First run with the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the values of the default parameters\n",
    "params1 = defaultParameters()\n",
    "\n",
    "### Run main function\n",
    "myModel1, fitData1, x_train1, y1C_train1, x_test1, y1C_test1 = mainFunction(params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot results\n",
    "showResults1(myModel1, fitData1, x_train1, y1C_train1, x_test1, y1C_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we change the default parameters and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the values of the default parameters\n",
    "params2 = copy.copy(params1)\n",
    "params2.LEARNING_RATE = 0.1\n",
    "\n",
    "### Run main function\n",
    "myModel2, fitData2, x_train2, y1C_train2, x_test2, y1C_test2 = mainFunction(params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showResults2(myModel1, fitData1, x_train1, y1C_train1, x_test1, y1C_test1,\n",
    "             myModel2, fitData2, x_train2, y1C_train2, x_test2, y1C_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a list of things that you can test:\n",
    "- Check that the results can be very different:\n",
    "  - For different runs with the same parameters (non-linear networks)\n",
    "  - By changing the value of several data parameters:\n",
    "    - TRAIN\\_SIZE\\_PCT\\_SPLIT: 0.70,0.20,0.01 (ionosphere, MLP-100-50, SGD, LR = 0.001)\n",
    "    - SCALE\\_INPUTS\\_FUNCTION (hepatits, linear or non-linear networks)\n",
    "  - By changing the value of several critical training parameters (ionosphere, MLP-100-50):\n",
    "    - TRAINING\\_ALGORITHM: SGD, RMSprop, Adam\n",
    "    - LEARNING\\_RATE: 0.1,0.01,0.001,0.0001\n",
    "    - MOMENTUM\\_RATE: 0.80, 0.00 (SGD, LR = 0.001)\n",
    "    - FACTIVATION\\_HIDDEN1 and FACTIVATION\\_HIDDEN2: tanh, relu (SGD, LR = 0.001)\n",
    "    - FACTIVATION\\_OUTPUT: softmax, linear\n",
    "- Underfitting and overfitting:\n",
    "  - Underfitting:\n",
    "    - Train a linear model and compare with a non-linear one (xor)\n",
    "    - Train a non-linear model with small LR few epochs (ionosphere - MLP-100-50, SGD, LR <= 0.0001, EPOCHS = 200)\n",
    "  - Overfitting:\n",
    "    - Train a non-linear model with large LR few epochs (ionosphere - MLP-100-50, SGD, LR >= 0.01, EPOCHS = 200)\n",
    "    - Train a non-linear model with small LR many epochs (ionosphere - MLP-100-50, SGD, LR <= 0.0001, EPOCHS = 5000)\n",
    "    - Train a non-linear model with many hidden layers (sonar - MLP-20 vs MLP-20-20-20-20-20, SGD, LR = 0.002,   EPOCHS = 200)\n",
    "    - Curse of Dimensionality: small value for TRAIN\\_SIZE\\_PCT\\_SPLIT (ionosphere - MLP-100-50, SGD, LR = 0.01, EPOCHS = 200)\n",
    "- The effect of the noise in the data:\n",
    "  - Add random inputs and labels: NRANDOM_EXAMPLES = 1000 (ionosphere - MLP-100-50, SGD, LR = 0.001, EPOCHS = 200)\n",
    "  - Add random features: NRANDOM_FEATURES = 1000,10000 (ionosphere - MLP-100-50, SGD, LR = 0.001, EPOCHS = 200)\n",
    "  - Add noise to the inputs: ADD_NOISE_INPUTS = True\n",
    "  - Shuffle the labels: SHUFFLE\\_LABELS = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
