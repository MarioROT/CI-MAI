{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data sets:\n",
    "### 1: Two linearly separable classes\n",
    "### 2: XOR data\n",
    "### 3: Sklearn moons\n",
    "### 4: Sklearn circles\n",
    "NDataSet = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000 # Amount of data\n",
    "\n",
    "### Constructs the dataset\n",
    "if   NDataSet == 1:\n",
    "    n = N // 2\n",
    "    ### Gaussian input values for both dimensions / Class 1 with an offset in the second one\n",
    "    X0 = np.random.randn(n,2)\n",
    "    Y0 = np.zeros(n)\n",
    "    #\n",
    "    offset = 7.0\n",
    "    X1 = np.random.randn(n,2) + offset\n",
    "    Y1 = np.ones(n)\n",
    "    #\n",
    "    X = np.vstack([X0,X1])\n",
    "    Y = np.hstack([Y0,Y1])\n",
    "elif NDataSet == 2:\n",
    "    n = N // 4\n",
    "    ### Uniform input values for both dimensions\n",
    "    offset = 4.0\n",
    "    X0A = np.hstack([np.random.randn(n,1) - offset, np.random.randn(n,1) + offset])\n",
    "    Y0A = np.zeros(n)\n",
    "    X0B = np.hstack([np.random.randn(n,1) + offset, np.random.randn(n,1) - offset])\n",
    "    Y0B = np.zeros(n)\n",
    "    X1A = np.hstack([np.random.randn(n,1) + offset, np.random.randn(n,1) + offset])\n",
    "    Y1A = np.ones(n)\n",
    "    X1B = np.hstack([np.random.randn(n,1) - offset, np.random.randn(n,1) - offset])\n",
    "    Y1B = np.ones(n)\n",
    "    X = np.vstack([X0A,X0B,X1A,X1B])\n",
    "    Y = np.hstack([Y0A,Y0B,Y1A,Y1B])\n",
    "elif NDataSet == 3:\n",
    "    from sklearn.datasets import make_moons\n",
    "    X, Y = make_moons(n_samples = N, noise = 0.05)\n",
    "elif NDataSet == 4:\n",
    "    from sklearn.datasets import make_circles\n",
    "    X, Y = make_circles(n_samples = N, noise = 0.05, factor = 0.5)\n",
    "else:\n",
    "    ### In sklearn there are other artificial data sets (see make_classification, for example)\n",
    "    print(\"NDataSet not implemented\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scatter plot with the data\n",
    "idxYclass0 = (Y[:] == 0)\n",
    "idxYclass1 = (Y[:] == 1)\n",
    "#print(type(X[idxYclass0, 0]), type(X[idxYclass0, 1]))\n",
    "#print(len(X[idxYclass0, 0]), len(X[idxYclass0, 1]))\n",
    "#print(X[idxYclass0, 0], X[idxYclass0, 1])\n",
    "plt.scatter(X[idxYclass0, 0], X[idxYclass0, 1], s=3, c='blue') #outer circle \n",
    "plt.scatter(X[idxYclass1, 0], X[idxYclass1, 1], s=3, c='red')  #inner circle \n",
    "plt.axis('auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "minX0 = np.min(X[:,0])\n",
    "maxX0 = np.max(X[:,0])\n",
    "minX1 = np.min(X[:,1])\n",
    "maxX1 = np.max(X[:,1])\n",
    "print(minX0,maxX0,minX1,maxX1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct (hand-made) reasonable solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### In some cases we can construct directly a solution\n",
    "###\n",
    "\n",
    "### This function will plot the output surface\n",
    "def plotNetOutputFDataset(X,Y,strF):\n",
    "    \n",
    "    ### Indexes of the examples in every class (WE ARE ASSUMING WE ONLY HAVE 2 CLASSES)\n",
    "    idxYclass0 = (Y[:] == 0)\n",
    "    idxYclass1 = (Y[:] == 1)\n",
    "    ### Minimum and maximum values of the input data\n",
    "    minX0 = np.min(X[:,0])\n",
    "    maxX0 = np.max(X[:,0])\n",
    "    minX1 = np.min(X[:,1])\n",
    "    maxX1 = np.max(X[:,1])\n",
    "    \n",
    "    ### Predictions surface\n",
    "    NMesh = 50\n",
    "    _x0 = np.linspace(minX0-0.2, maxX0+0.2, NMesh)\n",
    "    _x1 = np.linspace(minX1-0.2, maxX1+0.2, NMesh)\n",
    "    _Y  = np.zeros([NMesh, NMesh])\n",
    "\n",
    "    for i0, x0 in enumerate(_x0):\n",
    "        for i1, x1 in enumerate(_x1):\n",
    "            _Xtmp = np.array([[x0, x1]])\n",
    "            _Y[i1, i0] = Y = eval(strF)(_Xtmp)\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(5,5))\n",
    "    ax  = plt.axes()\n",
    "    #\n",
    "    ax.pcolormesh(_x0, _x1, _Y, cmap='coolwarm', shading='auto')\n",
    "    ax.axis('equal')\n",
    "    ax.scatter(X[idxYclass0, 0], X[idxYclass0, 1], s=3, c='skyblue')\n",
    "    ax.scatter(X[idxYclass1, 0], X[idxYclass1, 1], s=3, c='salmon')\n",
    "    #\n",
    "    return\n",
    "\n",
    "###\n",
    "\n",
    "def logisticFun(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def fDataSet1(x):       ### For this data set we only need one (output) unit\n",
    "    W = np.array([1,1])\n",
    "    b = -7.0\n",
    "    netInput = np.dot(x,W) + b\n",
    "    netOutput = logisticFun(netInput)\n",
    "    return netOutput\n",
    "\n",
    "def fDataSet2(x):       ### For this data set we only need two hidden units and one output unit\n",
    "    W11 = np.array([+0.3,+0.3])\n",
    "    b11 = +0.5\n",
    "    W12 = np.array([+0.3,+0.3])\n",
    "    b12 = -0.5\n",
    "    W21 = np.array([-1,+1])\n",
    "    b21 = +0.\n",
    "    #\n",
    "    netInput11 = np.dot(x,W11) + b11\n",
    "    netOutput11 = logisticFun(netInput11)\n",
    "    netInput12 = np.dot(x,W12) + b12\n",
    "    netOutput12 = logisticFun(netInput12)\n",
    "    #\n",
    "    x2 = np.array([netOutput11,netOutput12]).T\n",
    "    netInput21 = np.dot(x2,W21) + b21\n",
    "    netOutput21 = logisticFun(netInput21)\n",
    "    #\n",
    "    return netOutput21    \n",
    "\n",
    "### Now call the proper function depending on the data set\n",
    "if NDataSet == 1 or NDataSet == 2:\n",
    "    if   NDataSet == 1:\n",
    "        strF = \"fDataSet1\"\n",
    "    elif NDataSet == 2:\n",
    "        strF = \"fDataSet2\"\n",
    "    #\n",
    "    plotNetOutputFDataset(X,Y,strF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train step by step and plot of the training process\n",
    "def trainModelSteps(neural_n,X,Y,nEpochs,plotEvery):\n",
    "\n",
    "    import time\n",
    "    from IPython.display import clear_output\n",
    "    from ipywidgets.widgets.interaction import show_inline_matplotlib_plots\n",
    "\n",
    "    ### Indexes of the examples in every class (WE ARE ASSUMING WE ONLY HAVE 2 CLASSES)\n",
    "    idxYclass0 = (Y[:] == 0)\n",
    "    idxYclass1 = (Y[:] == 1)\n",
    "    ### Minimum and maximum values of the input data\n",
    "    minX0 = np.min(X[:,0])\n",
    "    maxX0 = np.max(X[:,0])\n",
    "    minX1 = np.min(X[:,1])\n",
    "    maxX1 = np.max(X[:,1])\n",
    "    \n",
    "    ### Initial fit (fit starts always from scratch and initializes all internal variables)\n",
    "    neural_n.fit(X,Y)\n",
    "\n",
    "    Accuracy = []\n",
    "    for i in range(nEpochs):\n",
    "\n",
    "        sX, sY = X, Y\n",
    "        #Maybe we need this (https://stackoverflow.com/questions/24617356/sklearn-sgdclassifier-partial-fit)\n",
    "        # to simulate shuffle=True (partial_fit does not shuffle?)\n",
    "        #sR = [i for i in range(len(X))]\n",
    "        #import random\n",
    "        #random.shuffle(sR)\n",
    "        #sX = [X[i] for i in sR]\n",
    "        #sY = [Y[i] for i in sR]\n",
    "\n",
    "        ### Partial fit (updates the model one iteration)\n",
    "        neural_n.partial_fit(sX,sY,np.unique(sY))\n",
    "        #print(neural_n.loss_curve_)\n",
    "\n",
    "        ### Loss value (the loss_curve_ variable contains the whole history)\n",
    "        Loss = neural_n.loss_curve_\n",
    "\n",
    "        ### Accuracy\n",
    "        current_accuracy = neural_n.score(X,Y)\n",
    "        Accuracy.append(current_accuracy)\n",
    "\n",
    "        if i % plotEvery ==0:\n",
    "\n",
    "            ### Predictions surface\n",
    "            NMesh = 50\n",
    "            _x0 = np.linspace(minX0-0.2, maxX0+0.2, NMesh)\n",
    "            _x1 = np.linspace(minX1-0.2, maxX1+0.2, NMesh)\n",
    "            _Y  = np.zeros([NMesh, NMesh])\n",
    "\n",
    "            for i0, x0 in enumerate(_x0):\n",
    "                for i1, x1 in enumerate(_x1):\n",
    "                    _Xtmp = np.array([[x0, x1]])\n",
    "                    _Y[i1, i0] = neural_n.predict_proba(_Xtmp)[0][1]\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            # Matplotlib - How to Change Subplot Sizes\n",
    "            #  https://blog.finxter.com/matplotlib-how-to-change-subplot-sizes/\n",
    "            #fig, ax = plt.subplots(1, 3, figsize=(15,5), gridspec_kw={'width_ratios': [2, 0.8, 0.8]})\n",
    "            fig = plt.figure(constrained_layout=True, figsize=(10,10))\n",
    "            ax = fig.add_gridspec(16, 16)\n",
    "            ax0 = fig.add_subplot(ax[0:8,  0:16])\n",
    "            ax1 = fig.add_subplot(ax[8:16, 0:8])\n",
    "            ax2 = fig.add_subplot(ax[8:16, 8:16])\n",
    "            #\n",
    "            ax0.pcolormesh(_x0, _x1, _Y, cmap='coolwarm', shading='auto')\n",
    "            ax0.axis('equal')\n",
    "            ax0.scatter(X[idxYclass0, 0], X[idxYclass0, 1], s=3, c='skyblue')\n",
    "            ax0.scatter(X[idxYclass1, 0], X[idxYclass1, 1], s=3, c='salmon')\n",
    "            #\n",
    "            ax1.plot(range(len(Loss)), Loss)\n",
    "            ax1.set_xlabel('Epochs')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            #\n",
    "            ax2.plot(range(len(Accuracy)), Accuracy)\n",
    "            ax2.set_xlabel('Epochs')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            #\n",
    "            plt.show()\n",
    "            show_inline_matplotlib_plots()\n",
    "\n",
    "            ## Figure without subplots...\n",
    "            ##plt.pcolormesh(_x0, _x1, _Y, cmap='coolwarm', shading='auto')\n",
    "            ##plt.axis('equal')\n",
    "            ##plt.scatter(X[idxYclass0, 0], X[idxYclass0, 1], c='skyblue')\n",
    "            ##plt.scatter(X[idxYclass1, 0], X[idxYclass1, 1], c='salmon')\n",
    "            ##plt.show()\n",
    "            ##\n",
    "            ##plt.plot(range(len(Loss)), Loss)\n",
    "            ##plt.xlabel('Epochs')\n",
    "            ##plt.ylabel('Loss')\n",
    "            ##plt.show()\n",
    "            ##\n",
    "            ##plt.plot(range(len(Accuracy)), Accuracy)\n",
    "            ##plt.xlabel('Epochs')\n",
    "            ##plt.ylabel('Accuracy')\n",
    "            ##plt.show()\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    print('Loss at the end of the training:',Loss[-1])\n",
    "    print('Accuracy at the end of the training:',Accuracy[-1])\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition of the MLPClassifier (from sklearn)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "HSizes     = ()         # Tuple: the ith element represents the number of neurons in the ith hidden layer\n",
    "ActFun     = 'tanh'     # 'logistic' 'tanh' 'relu'\n",
    "Solver     = 'sgd'      # 'sgd' 'adam'\n",
    "LRate_init = 0.01       # 0.001\n",
    "Momentum   = 0.9        # 0.9\n",
    "LRate_schm = 'constant'\n",
    "L2Reg      = 0.0        # 0.0001\n",
    "EarlyStop  = False\n",
    "MaxIterIni = 1          # We want to train epoch by epoch, so we perform an initial training of just 1 iteration\n",
    "MaxEpochs  = 500        # This parameter should be the value of max_iter in a standard definition\n",
    "\n",
    "neural_n = MLPClassifier(hidden_layer_sizes=HSizes, activation=ActFun, solver=Solver, alpha=L2Reg,\\\n",
    "                         learning_rate=LRate_schm, learning_rate_init=LRate_init, momentum=Momentum,\\\n",
    "                         early_stopping=EarlyStop, max_iter=MaxIterIni)\n",
    "\n",
    "### General comments\n",
    "### - 'adam' is usually faster than 'sdg' (specially for larger networks)\n",
    "### - Very small learning rates (say, 0.00001) make the training process slow\n",
    "### - Very large learning rates (say, 100.0)   make the training process degenerate\n",
    "### - The momentum term accelerates the training process\n",
    "\n",
    "###\n",
    "### If NDataSet == 1, try\n",
    "### - HSizes = ()    / ActFun = 'tanh' / Solver = 'sgd' / LRate_init = 0.001 / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (1)   / ActFun = 'tanh' / Solver = 'sgd' / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "###\n",
    "### If NDataSet == 2, try\n",
    "### - HSizes = (2)   / ActFun = 'tanh' / Solver = 'sgd' / LRate_init = 0.01 / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (5)   / ActFun = 'tanh' / Solver = 'sgd' / LRate_init = 0.01 / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (10)  / ActFun = 'tanh' / Solver = 'sgd' / LRate_init = 0.01 / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (50)  / ActFun = 'tanh' / Solver = 'sgd' / LRate_init = 0.01 / Momentum = 0.9 / MaxEpochs = 500\n",
    "###   ... and you will see different solutions for the same problem\n",
    "###\n",
    "### If NDataSet == 3, try\n",
    "### - HSizes = (20)  / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.001 / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (20)  / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (20)  / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.1   / Momentum = 0.9 / MaxEpochs = 500\n",
    "###   ... to see the effect of the *learning rate*\n",
    "### - HSizes = (20)  / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (20)  / ActFun = 'relu' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (200) / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (200) / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 1000\n",
    "###       (with ActFun = 'tanh', you will need \"MaxEpochs = 1000\")\n",
    "### - HSizes = (200) / ActFun = 'relu' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "###   ... to see the effect of the *activation function*\n",
    "### - HSizes = (50)  / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (50)  / ActFun = 'tanh' / Solver = 'adam' / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (200) / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (200) / ActFun = 'tanh' / Solver = 'adam' / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "###   ... to see the effect of the optimization algorithm*\n",
    "### - HSizes = (20)  / ActFun = 'tanh' / Solver = 'sgd'  / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "### - HSizes = (20)  / ActFun = 'relu' / Solver = 'adam' / LRate_init = 0.01  / Momentum = 0.9 / MaxEpochs = 500\n",
    "###   ... are nice solutions\n",
    "###\n",
    "### If NDataSet == 4, try\n",
    "### - The same configurations than for NDataSet = 3\n",
    "### - You will see a similar behaviour, but less smooth in some cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train and plot\n",
    "plotEvery = 25\n",
    "trainModelSteps(neural_n,X,Y,MaxEpochs,plotEvery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-kIkbyozElv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "First Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
